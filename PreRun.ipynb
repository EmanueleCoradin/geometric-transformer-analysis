{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081c9555-d877-4796-b843-ae2fdcc2b29f",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c7d6d7-25ea-4630-976d-d3aeedc25853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "\n",
    "# Transformers libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Model\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ec6cd-d163-46f2-8211-e4beff8ba012",
   "metadata": {},
   "source": [
    "### Definition of the model and the hook function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c49573-1d12-4ab2-812e-3d471bae6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'openai-community/gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, output_attentions=True)\n",
    "gpt2_model = model.transformer\n",
    "\n",
    "# Function to be called by the hook\n",
    "output_list, module_list = [], []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    output_list.append(output)\n",
    "    module_list.append(module)\n",
    "\n",
    "# Attaching hook to all layers\n",
    "for layer in model.modules():\n",
    "    layer.register_forward_hook(hook_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e82dc81-0eee-48d0-8bb7-39ec958ede11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprompt = \"Explain this poem: O Captain! my Captain! our fearful trip is done,\\nThe ship has weather’d every rack, the prize we sought is won,\\nThe port is near, the bells I hear, the people all exulting,\\nWhile follow eyes the steady keel, the vessel grim and daring;\\nBut O heart! heart! heart!\\nO the bleeding drops of red,\\nWhere on the deck my Captain lies,\\nFallen cold and dead.\\n\\nO Captain! my Captain! rise up and hear the bells;\\nRise up—for you the flag is flung—for you the bugle trills,\\nFor you bouquets and ribbon’d wreaths—for you the shores a-crowding,\\nFor you they call, the swaying mass, their eager faces turning;\\nHere Captain! dear father!\\nThis arm beneath your head!\\nIt is some dream that on the deck,\\nYou’ve fallen cold and dead.\\n\\nMy Captain does not answer, his lips are pale and still,\\nMy father does not feel my arm, he has no pulse nor will,\\nThe ship is anchor’d safe and sound, its voyage closed and done,\\nFrom fearful trip the victor ship comes in with object won;\\nExult O shores, and ring O bells!\\nBut I with mournful tread,\\nWalk the deck my Captain lies,\\nFallen cold and dead.\"\\n\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\n\\nwith torch.no_grad():\\n    outputs = model(input_ids)\\noutputs.logits.shape\\n# Get the logits for the last token in the input sequence\\nlogits = outputs.logits\\n\\n# Select the logits for the last token\\nlast_token_logits = logits[0, -1, :]\\n\\n# Apply softmax to get probabilities\\nprobabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\\n\\n# Get the predicted next token (the one with the highest probability)\\npredicted_token_id = torch.argmax(probabilities).item()\\n\\n# Convert the predicted token ID back to a token\\npredicted_token = tokenizer.decode([predicted_token_id])\\n\\nprint(f\"Predicted next token: \",predicted_token)\\npredicted_token_id\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just a test to understand the last passages, leave it there for the moment\n",
    "'''\n",
    "prompt = \"Explain this poem: O Captain! my Captain! our fearful trip is done,\\nThe ship has weather’d every rack, the prize we sought is won,\\nThe port is near, the bells I hear, the people all exulting,\\nWhile follow eyes the steady keel, the vessel grim and daring;\\nBut O heart! heart! heart!\\nO the bleeding drops of red,\\nWhere on the deck my Captain lies,\\nFallen cold and dead.\\n\\nO Captain! my Captain! rise up and hear the bells;\\nRise up—for you the flag is flung—for you the bugle trills,\\nFor you bouquets and ribbon’d wreaths—for you the shores a-crowding,\\nFor you they call, the swaying mass, their eager faces turning;\\nHere Captain! dear father!\\nThis arm beneath your head!\\nIt is some dream that on the deck,\\nYou’ve fallen cold and dead.\\n\\nMy Captain does not answer, his lips are pale and still,\\nMy father does not feel my arm, he has no pulse nor will,\\nThe ship is anchor’d safe and sound, its voyage closed and done,\\nFrom fearful trip the victor ship comes in with object won;\\nExult O shores, and ring O bells!\\nBut I with mournful tread,\\nWalk the deck my Captain lies,\\nFallen cold and dead.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "outputs.logits.shape\n",
    "# Get the logits for the last token in the input sequence\n",
    "logits = outputs.logits\n",
    "\n",
    "# Select the logits for the last token\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "# Get the predicted next token (the one with the highest probability)\n",
    "predicted_token_id = torch.argmax(probabilities).item()\n",
    "\n",
    "# Convert the predicted token ID back to a token\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "\n",
    "print(f\"Predicted next token: \",predicted_token)\n",
    "predicted_token_id\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5414660-e6ed-4a17-bab2-b9e3664401cc",
   "metadata": {},
   "source": [
    "### Choosing the prompt and running gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd223f1-d197-4583-b5fe-996b58c75950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "attn_output shape:  torch.Size([1, 997, 768])\n",
      "0 Embedding(50257, 768)    output_shape:  torch.Size([1, 997, 768])\n",
      "1 Embedding(1024, 768)    output_shape:  torch.Size([1, 997, 768])\n",
      "2 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "3 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "4 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "5 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "6 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "7 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "8 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "9 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "10 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "11 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "12 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "13 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "14 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "15 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "16 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "17 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "18 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "19 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "20 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "21 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "22 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "23 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "24 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "25 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "26 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "27 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "28 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "29 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "30 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "31 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "32 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "33 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "34 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "35 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "36 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "37 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "38 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "39 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "40 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "41 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "42 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "43 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "44 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "45 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "46 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "47 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "48 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "49 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "50 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "51 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "52 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "53 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "54 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "55 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "56 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "57 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "58 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "59 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "60 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "61 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "62 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "63 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "64 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "65 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "66 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "67 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "68 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "69 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "70 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "71 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "72 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "73 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "74 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "75 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "76 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "77 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "78 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "79 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "80 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "81 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "82 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "83 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "84 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "85 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "86 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "87 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "88 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "89 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "90 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "91 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "92 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "93 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "94 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "95 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "96 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "97 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "98 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "99 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "100 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "101 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "102 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "103 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "104 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "105 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "106 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "107 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "108 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "109 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "110 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "111 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "112 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "113 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "114 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "115 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "116 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "117 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "118 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "119 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "120 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "121 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "122 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "123 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "124 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "125 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "126 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "127 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "128 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "129 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "130 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "131 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "132 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "133 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "134 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "135 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "136 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "137 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "138 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "139 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "140 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "141 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "142 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "143 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "144 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "145 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "146 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "147 Conv1D()    output_shape:  torch.Size([1, 997, 2304])\n",
      "148 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 12, 997, 997])\n",
      "149 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "150 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "151 GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  3\n",
      "152 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "153 Conv1D()    output_shape:  torch.Size([1, 997, 3072])\n",
      "154 NewGELUActivation()    output_shape:  torch.Size([1, 997, 3072])\n",
      "155 Conv1D()    output_shape:  torch.Size([1, 997, 768])\n",
      "156 Dropout(p=0.1, inplace=False)    output_shape:  torch.Size([1, 997, 768])\n",
      "157 GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")    output_shape:  torch.Size([1, 997, 768])\n",
      "158 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")    output_shape:  3\n",
      "159 LayerNorm((768,), eps=1e-05, elementwise_affine=True)    output_shape:  torch.Size([1, 997, 768])\n",
      "160 GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")    output_shape:  3\n",
      "161 Linear(in_features=768, out_features=50257, bias=False)    output_shape:  torch.Size([1, 997, 50257])\n",
      "162 GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")    output_shape:  3\n"
     ]
    }
   ],
   "source": [
    "# About life (Tokens: 7)\n",
    "#prompt = \"What's the meaning of life?\"\n",
    "\n",
    "# About Walt Whitman (Tokens: 308)\n",
    "#prompt = \"Explain this poem: O Captain! my Captain! our fearful trip is done,\\nThe ship has weather’d every rack, the prize we sought is won,\\nThe port is near, the bells I hear, the people all exulting,\\nWhile follow eyes the steady keel, the vessel grim and daring;\\nBut O heart! heart! heart!\\nO the bleeding drops of red,\\nWhere on the deck my Captain lies,\\nFallen cold and dead.\\n\\nO Captain! my Captain! rise up and hear the bells;\\nRise up—for you the flag is flung—for you the bugle trills,\\nFor you bouquets and ribbon’d wreaths—for you the shores a-crowding,\\nFor you they call, the swaying mass, their eager faces turning;\\nHere Captain! dear father!\\nThis arm beneath your head!\\nIt is some dream that on the deck,\\nYou’ve fallen cold and dead.\\n\\nMy Captain does not answer, his lips are pale and still,\\nMy father does not feel my arm, he has no pulse nor will,\\nThe ship is anchor’d safe and sound, its voyage closed and done,\\nFrom fearful trip the victor ship comes in with object won;\\nExult O shores, and ring O bells!\\nBut I with mournful tread,\\nWalk the deck my Captain lies,\\nFallen cold and dead.\"\n",
    "\n",
    "# About cats (Tokens: 559)\n",
    "# prompt = \"Cats, the enigmatic and beloved companions of humans for thousands of years, have always held a special place in our hearts and homes. With their graceful movements, expressive eyes, and independent spirits, cats captivate us with their unique charm and personalities. Domestic cats, scientifically known as Felis catus, are descendants of wild cats that were domesticated by ancient civilizations. While their exact origins remain shrouded in mystery, it is believed that cats first began living alongside humans in agricultural societies, where they proved invaluable in controlling rodent populations that threatened food stores. One of the most striking features of cats is their remarkable agility and athleticism. With their flexible bodies, keen reflexes, and razor-sharp claws, cats are natural-born hunters capable of stalking prey with unparalleled precision. Whether prowling through tall grasses in search of mice or pouncing on a feather toy in the living room, cats exhibit a predatory instinct that harkens back to their wild ancestry. Beyond their hunting prowess, cats are also renowned for their unique vocalizations and communicative behaviors. From the plaintive meow of a hungry kitten to the contented purr of a cat curled up in its favorite spot, felines use a diverse array of sounds and body language to express their needs, emotions, and desires. Each cat develops its own distinct vocal repertoire and personality, endearing themselves to their human companions with their individual quirks and mannerisms. Cats are also revered for their mysterious and independent nature. Unlike dogs, which often crave constant attention and companionship, cats are solitary creatures that value their autonomy and personal space. While they may form strong bonds with their human caregivers, cats also enjoy exploring the world at their own pace, indulging in moments of solitary contemplation, and retreating to quiet corners for rest and relaxation. In addition to their role as beloved pets, cats have also been revered and mythologized in cultures around the world. Ancient Egyptians worshipped cats as symbols of grace, fertility, and protection, with the goddess Bastet often depicted as a lioness or domestic cat. In Japanese folklore, the beckoning cat or 'Maneki-neko' is believed to bring good luck and fortune to its owner, while in European folklore, cats were sometimes associated with witchcraft and superstition. Today, cats continue to enchant and inspire us with their beauty, intelligence, and innate curiosity. Whether lounging in sunbeams, engaging in playful antics, or simply offering a comforting presence during times of need, cats have a way of leaving an indelible mark on our lives and reminding us of the simple joys of companionship and connection. As we celebrate these extraordinary creatures, let us cherish the special bond we share with our feline friends and honor the timeless legacy of the cat throughout history.\"\n",
    "\n",
    "# About physics and music (Tokens: 997)\n",
    "prompt = \"Douglas Hofstadter’s “Gödel, Escher, Bach: An Eternal Golden Braid” (often abbreviated as GEB) is a unique and profound exploration of the interplay between mathematics, art, and music, presented through the lenses of three remarkable figures: the logician Kurt Gödel, the artist M.C. Escher, and the composer Johann Sebastian Bach. This Pulitzer Prize-winning work weaves together themes from various disciplines, creating a tapestry that examines the nature of human cognition, the essence of creativity, and the fundamental principles of self-reference and formal systems. Gödel’s Incompleteness Theorems: Kurt Gödel, an Austrian logician, is perhaps best known for his incompleteness theorems. These theorems fundamentally changed the landscape of mathematics and logic in the 20th century. Gödel’s first incompleteness theorem states that in any sufficiently powerful formal system, there are propositions that cannot be proven or disproven within the system. In other words, there are true statements that the system cannot prove. This theorem implies a limitation on the power of formal mathematical systems, revealing that there are truths that transcend formal proof. Hofstadter delves deeply into Gödel’s proof, illustrating how it employs a form of self-reference, similar to the paradoxes of Epimenides and the Liar. Gödel’s ingenious method involved encoding statements about the system within the system itself, leading to a statement that essentially says, “This statement is unprovable within this system.” If the system could prove this statement, it would lead to a contradiction, thereby establishing the truth of the statement outside the system’s proving capabilities. Escher’s Artistic Paradoxes: M.C. Escher, a Dutch graphic artist, is renowned for his mathematically inspired woodcuts, lithographs, and mezzotints, which feature impossible constructions, explorations of infinity, and intricate tessellations. Escher’s artwork often plays with perspective and optical illusion, creating images that defy conventional logic and evoke a sense of wonder and curiosity. Hofstadter draws parallels between Escher’s visual paradoxes and Gödel’s logical paradoxes. Escher’s works like “Drawing Hands,” where two hands are drawing each other, or “Relativity,” where multiple gravity-defying staircases coexist, exemplify the concept of self-reference and recursive structures. These artworks challenge our perception of reality and force us to reconsider the boundaries between the possible and the impossible. Bach’s Musical Patterns: Johann Sebastian Bach, the German composer and musician of the Baroque period, is celebrated for his intricate compositions and mastery of counterpoint. Hofstadter examines Bach’s use of fugues, canons, and other forms of musical recursion to illustrate patterns and structures that resonate with the themes of self-reference and formal systems. Bach’s music, with its interweaving melodies and harmonious complexity, serves as a metaphor for the layered and recursive nature of formal systems. Hofstadter particularly highlights Bach’s “Musical Offering,” a collection of compositions based on a single musical theme that is transformed and developed in various ingenious ways. This mirrors the idea of taking a simple formal system and exploring its possibilities through recursive applications and transformations. The Concept of Strange Loops: A central theme in GEB is the concept of “strange loops,” a term Hofstadter uses to describe a hierarchical structure that loops back on itself. This idea is illustrated through Gödel’s theorems, Escher’s artwork, and Bach’s music. A strange loop occurs when, by moving through different levels of a system, one unexpectedly arrives back at the starting point. Hofstadter argues that strange loops are at the core of consciousness and self-awareness. He suggests that the human mind is a complex, recursive system capable of self-reference, and this ability to reflect upon itself is what gives rise to the phenomenon of “I.” Our thoughts can contemplate themselves, creating a loop of self-reference that is both a source of paradox and a hallmark of intelligence. Formal Systems and Symbolic Representation: Hofstadter explores the nature of formal systems, which are systems of rules and symbols used to produce statements and derive conclusions. He discusses how formal systems can represent complex ideas and how they relate to the way we think and communicate. The notion of symbolic representation is crucial in understanding how abstract concepts can be encoded and manipulated within a formal system. The book frequently uses dialogues between fictional characters, such as Achilles and the Tortoise, to explore these concepts in an accessible and engaging manner.\"\n",
    "\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    \n",
    "for i, module in enumerate(module_list):\n",
    "    try:\n",
    "        print(i, module, '   output_shape: ', output_list[i].shape)\n",
    "    except:\n",
    "        try:\n",
    "            print(i, module, '   output_shape: ', len(output_list[i]))\n",
    "        except:\n",
    "            print(i, module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3c1e3-440c-4b47-8e3a-50f21382039f",
   "metadata": {},
   "source": [
    "### Storing output list and module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa014679-d5c8-4f15-8515-8a5d2932adf1",
   "metadata": {},
   "source": [
    "#### Word2Vec conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca243823-0197-4dc2-9ebd-43d5bdae1f41",
   "metadata": {},
   "source": [
    "Remember that the passages are:\n",
    "    \n",
    "    1. Convert word to tokens and tokens to vectors\n",
    "    2. Apply the positional encoding matrix\n",
    "    3. Sum the positional encoding to the vectorial representation of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fade4264-e634-410c-9790-9c1ee67bbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Token2Vec          = output_list[0][0]\n",
    "PositionalEncoding = output_list[1][0]\n",
    "PositionPlusVect   = output_list[2][0]\n",
    "\n",
    "torch.save(Token2Vec, \"output/word2vec/Token2Vec.pt\")\n",
    "torch.save(PositionalEncoding, \"output/word2vec/PositionalEncoding.pt\")\n",
    "torch.save(PositionPlusVect, \"output/word2vec/PositionPlusVect.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91101dad-6bd6-4fac-9a59-e50a543a1e89",
   "metadata": {},
   "source": [
    "#### Storing evolution of the decoder blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb4716d-1b2e-4916-9168-440441f0bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder  1   FirstNormalization\n",
      "Decoder  1   QKV_representation\n",
      "Decoder  1   AttentionHeads\n",
      "Decoder  1   AttentionProj\n",
      "Decoder  1   SecondNormalization\n",
      "Decoder  1   FirstLayerNN\n",
      "Decoder  1   SecondLayerNN\n",
      "Decoder  1   Decoder_Final_Output\n",
      "Decoder  2   FirstNormalization\n",
      "Decoder  2   QKV_representation\n",
      "Decoder  2   AttentionHeads\n",
      "Decoder  2   AttentionProj\n",
      "Decoder  2   SecondNormalization\n",
      "Decoder  2   FirstLayerNN\n",
      "Decoder  2   SecondLayerNN\n",
      "Decoder  2   Decoder_Final_Output\n",
      "Decoder  3   FirstNormalization\n",
      "Decoder  3   QKV_representation\n",
      "Decoder  3   AttentionHeads\n",
      "Decoder  3   AttentionProj\n",
      "Decoder  3   SecondNormalization\n",
      "Decoder  3   FirstLayerNN\n",
      "Decoder  3   SecondLayerNN\n",
      "Decoder  3   Decoder_Final_Output\n",
      "Decoder  4   FirstNormalization\n",
      "Decoder  4   QKV_representation\n",
      "Decoder  4   AttentionHeads\n",
      "Decoder  4   AttentionProj\n",
      "Decoder  4   SecondNormalization\n",
      "Decoder  4   FirstLayerNN\n",
      "Decoder  4   SecondLayerNN\n",
      "Decoder  4   Decoder_Final_Output\n",
      "Decoder  5   FirstNormalization\n",
      "Decoder  5   QKV_representation\n",
      "Decoder  5   AttentionHeads\n",
      "Decoder  5   AttentionProj\n",
      "Decoder  5   SecondNormalization\n",
      "Decoder  5   FirstLayerNN\n",
      "Decoder  5   SecondLayerNN\n",
      "Decoder  5   Decoder_Final_Output\n",
      "Decoder  6   FirstNormalization\n",
      "Decoder  6   QKV_representation\n",
      "Decoder  6   AttentionHeads\n",
      "Decoder  6   AttentionProj\n",
      "Decoder  6   SecondNormalization\n",
      "Decoder  6   FirstLayerNN\n",
      "Decoder  6   SecondLayerNN\n",
      "Decoder  6   Decoder_Final_Output\n",
      "Decoder  7   FirstNormalization\n",
      "Decoder  7   QKV_representation\n",
      "Decoder  7   AttentionHeads\n",
      "Decoder  7   AttentionProj\n",
      "Decoder  7   SecondNormalization\n",
      "Decoder  7   FirstLayerNN\n",
      "Decoder  7   SecondLayerNN\n",
      "Decoder  7   Decoder_Final_Output\n",
      "Decoder  8   FirstNormalization\n",
      "Decoder  8   QKV_representation\n",
      "Decoder  8   AttentionHeads\n",
      "Decoder  8   AttentionProj\n",
      "Decoder  8   SecondNormalization\n",
      "Decoder  8   FirstLayerNN\n",
      "Decoder  8   SecondLayerNN\n",
      "Decoder  8   Decoder_Final_Output\n",
      "Decoder  9   FirstNormalization\n",
      "Decoder  9   QKV_representation\n",
      "Decoder  9   AttentionHeads\n",
      "Decoder  9   AttentionProj\n",
      "Decoder  9   SecondNormalization\n",
      "Decoder  9   FirstLayerNN\n",
      "Decoder  9   SecondLayerNN\n",
      "Decoder  9   Decoder_Final_Output\n",
      "Decoder  10   FirstNormalization\n",
      "Decoder  10   QKV_representation\n",
      "Decoder  10   AttentionHeads\n",
      "Decoder  10   AttentionProj\n",
      "Decoder  10   SecondNormalization\n",
      "Decoder  10   FirstLayerNN\n",
      "Decoder  10   SecondLayerNN\n",
      "Decoder  10   Decoder_Final_Output\n",
      "Decoder  11   FirstNormalization\n",
      "Decoder  11   QKV_representation\n",
      "Decoder  11   AttentionHeads\n",
      "Decoder  11   AttentionProj\n",
      "Decoder  11   SecondNormalization\n",
      "Decoder  11   FirstLayerNN\n",
      "Decoder  11   SecondLayerNN\n",
      "Decoder  11   Decoder_Final_Output\n",
      "Decoder  12   FirstNormalization\n",
      "Decoder  12   QKV_representation\n",
      "Decoder  12   AttentionHeads\n",
      "Decoder  12   AttentionProj\n",
      "Decoder  12   SecondNormalization\n",
      "Decoder  12   FirstLayerNN\n",
      "Decoder  12   SecondLayerNN\n",
      "Decoder  12   Decoder_Final_Output\n"
     ]
    }
   ],
   "source": [
    "index_list = np.array([3, 4, 5, 7, 9, 11, 13, 15])\n",
    "'''\n",
    "Decoder_01_FirstNormalization  = output_list[3]\n",
    "Decoder_01_QKV_representation  = output_list[4]\n",
    "Decoder_01_AttentionHeads      = output_list[5]\n",
    "Decoder_01_AttentionProj       = output_list[7]\n",
    "Decoder_01_SecondNormalization = output_list[9]\n",
    "Decoder_01_FirstLayerNN        = output_list[11]\n",
    "Decoder_01_SecondLayerNN       = output_list[13]  'Delta space'\n",
    "Decoder_01_final_output        = output_list[15]  'Residual + Delta space'\n",
    "'''\n",
    "module_name = [\"FirstNormalization\", \"QKV_representation\", \"AttentionHeads\", \"AttentionProj\", \"SecondNormalization\", \"FirstLayerNN\", \"SecondLayerNN\", \"Decoder_Final_Output\"]\n",
    "# Create Decoder_mask and flatten it\n",
    "Decoder_mask = np.concatenate([index_list + i*13 for i in range(12)])\n",
    "\n",
    "# Assuming output_list is already defined, we can proceed\n",
    "# Extract elements for Decoder_list\n",
    "Decoder_list = [output_list[mask] for mask in Decoder_mask]\n",
    "\n",
    "PositionalEmbedding = output_list[2]\n",
    "\n",
    "Decoder_list = [output_list[mask] for mask in Decoder_mask]\n",
    "\n",
    "for i in range(12):\n",
    "    for j in range(8):\n",
    "        print(\"Decoder \", i + 1 , \" \", module_name[j])\n",
    "        torch.save(Decoder_list[j+i*8], \"output/decoder/decoder_\"+str(i+1) +\"/\"+module_name[j]+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cbb89c-7407-46f0-b15d-7f96bc6d06aa",
   "metadata": {},
   "source": [
    "Let's extract also the intermediate result of Output Attention + Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda41f71-1b52-4d84-9c04-b1083f185e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract also Output Attention + Residual connection\n",
    "\n",
    "SecondLayerNN_list         =  [torch.load(f\"output/decoder/decoder_{i+1}/SecondLayerNN.pt\")[0] for i in range(12)]\n",
    "Decoder_Final_Output_list  =  [torch.load(f\"output/decoder/decoder_{i+1}/Decoder_Final_Output.pt\")[0][0] for i in range(12)]\n",
    "\n",
    "AttentionPlusResidual_list =  [DecOut - SecLayer  for DecOut, SecLayer in zip(Decoder_Final_Output_list, SecondLayerNN_list)]\n",
    "\n",
    "for i, AttentionPlusResidual in enumerate(AttentionPlusResidual_list):\n",
    "    torch.save((AttentionPlusResidual.unsqueeze(0),)\n",
    ", f\"output/decoder/decoder_{i+1}/AttentionPlusResidual.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be27576-5875-4f73-98f9-53653c3f4d7c",
   "metadata": {},
   "source": [
    "Push Back the last token to the language domain to study the evolution of the probability distribution during the 12 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679eefc7-86a9-4d34-b99a-068363dd3fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder 1\n",
      "    Top10 attn:  tensor([35407, 16410,   685,   366, 49813,   198,  1002,   554,   383,  1649]) tensor([0.1031, 0.0733, 0.0671, 0.0662, 0.0324, 0.0230, 0.0180, 0.0176, 0.0140,\n",
      "        0.0118], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([ 2102,   198,   554,  1002,   632,  8447,   383, 50256, 28016,  1629]) tensor([0.1034, 0.0555, 0.0524, 0.0409, 0.0365, 0.0282, 0.0277, 0.0261, 0.0229,\n",
      "        0.0219], grad_fn=<TopkBackward0>)\n",
      "Decoder 2\n",
      "    Top10 attn:  tensor([ 2102,  1002,   554, 50256,   632,  8447,   198,   383, 28016,  8989]) tensor([0.0991, 0.0509, 0.0454, 0.0347, 0.0326, 0.0310, 0.0292, 0.0272, 0.0255,\n",
      "        0.0245], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([ 2102,   554,  1002,  8447, 50256,  8989,   383,   887,   198,   632]) tensor([0.1473, 0.0476, 0.0424, 0.0353, 0.0350, 0.0295, 0.0276, 0.0267, 0.0260,\n",
      "        0.0260], grad_fn=<TopkBackward0>)\n",
      "Decoder 3\n",
      "    Top10 attn:  tensor([ 2102, 50256,  1002,   554,  8989,   632,   198,   383, 11399,  1629]) tensor([0.1206, 0.0432, 0.0431, 0.0393, 0.0344, 0.0314, 0.0313, 0.0309, 0.0253,\n",
      "        0.0249], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([ 2102, 50256, 10968,   554, 11399,  1002,  8447,  8989, 12032, 15323]) tensor([0.1344, 0.0401, 0.0353, 0.0345, 0.0297, 0.0290, 0.0264, 0.0262, 0.0261,\n",
      "        0.0253], grad_fn=<TopkBackward0>)\n",
      "Decoder 4\n",
      "    Top10 attn:  tensor([ 2102,  8989, 50256, 10968, 28016,   383, 11399,  4900,   554,   632]) tensor([0.1456, 0.0420, 0.0354, 0.0315, 0.0314, 0.0301, 0.0284, 0.0273, 0.0272,\n",
      "        0.0271], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([ 2102,  8989, 10968,  8447, 11399,   632,   383,   554, 12032, 15298]) tensor([0.2432, 0.0511, 0.0377, 0.0342, 0.0334, 0.0296, 0.0266, 0.0235, 0.0228,\n",
      "        0.0214], grad_fn=<TopkBackward0>)\n",
      "Decoder 5\n",
      "    Top10 attn:  tensor([ 2102, 12032, 11399, 10968,  4900,  8989,  4418,   632,   383, 22660]) tensor([0.3186, 0.0807, 0.0509, 0.0437, 0.0402, 0.0307, 0.0279, 0.0250, 0.0215,\n",
      "        0.0212], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([ 2102, 12032, 11399, 10968,  4418,  8989, 22660, 15298,   632,   383]) tensor([0.3356, 0.1311, 0.0488, 0.0426, 0.0315, 0.0309, 0.0291, 0.0238, 0.0231,\n",
      "        0.0188], grad_fn=<TopkBackward0>)\n",
      "Decoder 6\n",
      "    Top10 attn:  tensor([ 2102, 12032,   632,   383,  4900, 11399, 22660,  8989,  4418,  2893]) tensor([0.3813, 0.1124, 0.0599, 0.0467, 0.0343, 0.0262, 0.0201, 0.0166, 0.0164,\n",
      "        0.0149], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([ 2102, 12032,  4900,  9461,  8989,   632, 11399,   383, 15298, 22660]) tensor([0.3519, 0.1093, 0.0402, 0.0379, 0.0324, 0.0294, 0.0287, 0.0263, 0.0257,\n",
      "        0.0196], grad_fn=<TopkBackward0>)\n",
      "Decoder 7\n",
      "    Top10 attn:  tensor([ 2102, 12032,  4900,   383,  9461,  8989,  2893, 11399, 36778, 15298]) tensor([0.4684, 0.1575, 0.0544, 0.0318, 0.0247, 0.0185, 0.0158, 0.0153, 0.0148,\n",
      "        0.0136], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([ 2102,  9461, 12032, 36778,  4900, 11399,  8989,   383, 15298, 10968]) tensor([0.3079, 0.2038, 0.1500, 0.1118, 0.0276, 0.0260, 0.0151, 0.0128, 0.0104,\n",
      "        0.0099], grad_fn=<TopkBackward0>)\n",
      "Decoder 8\n",
      "    Top10 attn:  tensor([  383,  9461,  2102, 36778, 14674, 12032,   554,   632,  4900,  3412]) tensor([0.2707, 0.2202, 0.0818, 0.0796, 0.0763, 0.0606, 0.0153, 0.0125, 0.0117,\n",
      "        0.0113], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([  383,  9461, 36778, 12032,  2102, 14674,   554, 15298,   770,  8013]) tensor([0.3105, 0.2306, 0.1307, 0.0752, 0.0650, 0.0250, 0.0182, 0.0102, 0.0092,\n",
      "        0.0085], grad_fn=<TopkBackward0>)\n",
      "Decoder 9\n",
      "    Top10 attn:  tensor([  383,  9461, 36778, 12032, 14674,  2102,   554,   770,  1081,  8013]) tensor([0.3686, 0.2357, 0.0693, 0.0658, 0.0579, 0.0292, 0.0239, 0.0111, 0.0094,\n",
      "        0.0093], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([ 9461,   383, 36778, 12032, 14674,   554,  2102,   770,  4418,  3819]) tensor([0.2918, 0.2173, 0.1855, 0.0795, 0.0406, 0.0242, 0.0179, 0.0124, 0.0063,\n",
      "        0.0061], grad_fn=<TopkBackward0>)\n",
      "Decoder 10\n",
      "    Top10 attn:  tensor([37745,   383,  9461,   554, 12032, 36778,   770, 14674,  1081,  3819]) tensor([0.3759, 0.2721, 0.0748, 0.0446, 0.0225, 0.0164, 0.0127, 0.0117, 0.0111,\n",
      "        0.0075], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([  383, 37745,   554,  9461, 12032,   770,  3819,  2102, 36778,  1081]) tensor([0.3564, 0.1962, 0.0760, 0.0578, 0.0426, 0.0198, 0.0160, 0.0146, 0.0137,\n",
      "        0.0136], grad_fn=<TopkBackward0>)\n",
      "Decoder 11\n",
      "    Top10 attn:  tensor([37745,   383, 25332,   554, 16319,   770,  9461,   632,  3819, 12032]) tensor([9.7141e-01, 9.3188e-03, 5.1785e-03, 1.5298e-03, 1.5219e-03, 6.7523e-04,\n",
      "        6.4360e-04, 4.8457e-04, 3.9172e-04, 3.1740e-04],\n",
      "       grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([37745,   383, 25332,   554,   770,  9461,   632, 16319,  2312,  3819]) tensor([9.4259e-01, 1.8722e-02, 1.1872e-02, 2.9136e-03, 2.3551e-03, 2.0394e-03,\n",
      "        1.1879e-03, 1.0673e-03, 9.4101e-04, 8.9397e-04],\n",
      "       grad_fn=<TopkBackward0>)\n",
      "Decoder 12\n",
      "    Top10 attn:  tensor([37745,   383, 25332, 16319,   198,   554,   770, 50182,   632,   317]) tensor([0.2893, 0.1059, 0.0380, 0.0250, 0.0229, 0.0207, 0.0151, 0.0150, 0.0084,\n",
      "        0.0084], grad_fn=<TopkBackward0>)\n",
      "     Top10 dec:  tensor([37745, 25332,   383, 16319,   198,   554, 50182,   770,   317,   632]) tensor([0.2107, 0.0757, 0.0740, 0.0380, 0.0269, 0.0201, 0.0187, 0.0178, 0.0158,\n",
      "        0.0098], grad_fn=<TopkBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Extract the final normalization layer and the 'inverse matrix'\n",
    "ln_f = gpt2_model.ln_f\n",
    "lm_head = model.lm_head\n",
    "\n",
    "# Let's save the 'projection' on the vocabulary before and after the softmax\n",
    "for i, (out_attention, out_decoder) in enumerate(zip(AttentionPlusResidual_list, Decoder_Final_Output_list)):\n",
    "    print(f\"Decoder {i+1}\")\n",
    "    \n",
    "    final_norm = ln_f(out_attention)\n",
    "    projection = lm_head(final_norm[-1])\n",
    "    softmax = F.softmax(projection, dim=-1)\n",
    "    \n",
    "    top_values, top_indices = torch.topk(softmax, 10, dim=-1)\n",
    "\n",
    "    print(\"    Top10 attn: \", top_indices, top_values)\n",
    "\n",
    "    torch.save(projection, f\"output/last_token_pdf/decoder_{i+1}/attention_projection.pt\")\n",
    "    torch.save(softmax, f\"output/last_token_pdf/decoder_{i+1}/attention_softmax.pt\")\n",
    "\n",
    "    final_norm = ln_f(out_decoder)\n",
    "    projection = lm_head(final_norm[-1])\n",
    "    softmax = F.softmax(projection, dim=-1)\n",
    "\n",
    "    torch.save(projection, f\"output/last_token_pdf/decoder_{i+1}/out_decoder_projection.pt\")\n",
    "    torch.save(softmax, f\"output/last_token_pdf/decoder_{i+1}/out_decoder_softmax.pt\")\n",
    "    \n",
    "    # Extract the top 10 entries with highest softmax values\n",
    "    top_values, top_indices = torch.topk(softmax, 10, dim=-1)\n",
    "    print(\"     Top10 dec: \", top_indices, top_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c4261a-123b-4ed1-b6be-afda721c84d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nln_f = gpt2_model.ln_f\\nlm_head = model.lm_head\\n\\nfinal_norm = ln_f(Decoder_Final_Output_list[-1])\\n# Get the output from the lm_head\\nprojection_1 = lm_head(final_norm[-1])\\n\\n# Apply softmax on projection_1\\nsoftmax_output = F.softmax(projection_1, dim=-1)\\n# Extract the top 10 entries with highest softmax values\\ntop_values, top_indices = torch.topk(softmax_output, 10, dim=-1)\\n\\nprint(\"Top 10 values:\")\\nprint(top_values)\\n\\nprint(\"Top 10 indices:\")\\nprint(top_indices)\\n\\nprint(softmax_output)\\n#print(projection_1-)\\ndiff = output_list[161][0][-1] - projection_1\\ndiff_soft = softmax_output - F.softmax(output_list[161][0][-1], dim=-1)\\nplaces = np.ones(diff.shape)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Template:\n",
    "'''\n",
    "ln_f = gpt2_model.ln_f\n",
    "lm_head = model.lm_head\n",
    "\n",
    "final_norm = ln_f(Decoder_Final_Output_list[-1])\n",
    "# Get the output from the lm_head\n",
    "projection_1 = lm_head(final_norm[-1])\n",
    "\n",
    "# Apply softmax on projection_1\n",
    "softmax_output = F.softmax(projection_1, dim=-1)\n",
    "# Extract the top 10 entries with highest softmax values\n",
    "top_values, top_indices = torch.topk(softmax_output, 10, dim=-1)\n",
    "\n",
    "print(\"Top 10 values:\")\n",
    "print(top_values)\n",
    "\n",
    "print(\"Top 10 indices:\")\n",
    "print(top_indices)\n",
    "\n",
    "print(softmax_output)\n",
    "#print(projection_1-)\n",
    "diff = output_list[161][0][-1] - projection_1\n",
    "diff_soft = softmax_output - F.softmax(output_list[161][0][-1], dim=-1)\n",
    "places = np.ones(diff.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2865ad51-cc5b-45fa-8b02-6077e50149f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
